#########################################################################################
#                                                                                       #
# Plot the learning curves                                                              #
# Start with the aggregated learning curves (i.e. multiple learning curves in one plot) #
#                                                                                       #
#########################################################################################
# Function to plot the learning curve.
#
# Inputs:
#   df - dataframe consisting of the columns "experiment" (indicating the dataset)
#        "num_train" (indicating the number of training examples) "reDistributpeat" (indicating a possible repeat of learning)
#        "algo" (indicating the algorithm) "tr" (indicating the experimentally measured retention time)
#        "pred" (indicating the prediction)
#
#   ylim - vector indicating the limits c([from,to])
#
#   legend_loc - vector with location of the legend c([relative location x, relative location y])
#
#   title - string with the title
#
#   xlab - string with the x label
#
#   ylab - string with the y label
#
#   percentage - boolean indicating if percentages are used (value will be multiplied by 100)
plot_learning_curves <- function(df,
ylim=c(-0.5,1.0),
legend_loc=c(0.95,0.75),
title="",
xlab="Number of initial training instances",
ylab="Average error relative to total elution time (%)",
percentage=F,
l1_selection=c("Layer 1","LASSO","BRR","SVR","AB","GB")){
# Percentages need to be multiplied by 100 %
if (percentage) {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf*100, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
else {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
# Apply a theme and put labels+legend in
p <- p + scale_fill_brewer("") + theme_classic() +
theme(plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
theme(axis.line.x = element_line(color="black", size = 0.5),
axis.line.y = element_line(color="black", size = 0.5),
axis.line = element_line(color="black", size = 0.5),
legend.justification=c(1,0), legend.position=legend_loc,legend.text=element_text(size=12))+
ylim(ylim)
p <- p + labs(title = title,
x = xlab,
y = ylab)
return(p)
}
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
library(ggplot2)
#########################################################################################
#                                                                                       #
# Plot the learning curves                                                              #
# Start with the aggregated learning curves (i.e. multiple learning curves in one plot) #
#                                                                                       #
#########################################################################################
# Function to plot the learning curve.
#
# Inputs:
#   df - dataframe consisting of the columns "experiment" (indicating the dataset)
#        "num_train" (indicating the number of training examples) "reDistributpeat" (indicating a possible repeat of learning)
#        "algo" (indicating the algorithm) "tr" (indicating the experimentally measured retention time)
#        "pred" (indicating the prediction)
#
#   ylim - vector indicating the limits c([from,to])
#
#   legend_loc - vector with location of the legend c([relative location x, relative location y])
#
#   title - string with the title
#
#   xlab - string with the x label
#
#   ylab - string with the y label
#
#   percentage - boolean indicating if percentages are used (value will be multiplied by 100)
plot_learning_curves <- function(df,
ylim=c(-0.5,1.0),
legend_loc=c(0.95,0.75),
title="",
xlab="Number of initial training instances",
ylab="Average error relative to total elution time (%)",
percentage=F,
l1_selection=c("Layer 1","LASSO","BRR","SVR","AB","GB")){
# Percentages need to be multiplied by 100 %
if (percentage) {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf*100, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
else {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
# Apply a theme and put labels+legend in
p <- p + scale_fill_brewer("") + theme_classic() +
theme(plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
theme(axis.line.x = element_line(color="black", size = 0.5),
axis.line.y = element_line(color="black", size = 0.5),
axis.line = element_line(color="black", size = 0.5),
legend.justification=c(1,0), legend.position=legend_loc,legend.text=element_text(size=12))+
ylim(ylim)
p <- p + labs(title = title,
x = xlab,
y = ylab)
return(p)
}
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
library(ggplot2)
#########################################################################################
#                                                                                       #
# Plot the learning curves                                                              #
# Start with the aggregated learning curves (i.e. multiple learning curves in one plot) #
#                                                                                       #
#########################################################################################
# Function to plot the learning curve.
#
# Inputs:
#   df - dataframe consisting of the columns "experiment" (indicating the dataset)
#        "num_train" (indicating the number of training examples) "reDistributpeat" (indicating a possible repeat of learning)
#        "algo" (indicating the algorithm) "tr" (indicating the experimentally measured retention time)
#        "pred" (indicating the prediction)
#
#   ylim - vector indicating the limits c([from,to])
#
#   legend_loc - vector with location of the legend c([relative location x, relative location y])
#
#   title - string with the title
#
#   xlab - string with the x label
#
#   ylab - string with the y label
#
#   percentage - boolean indicating if percentages are used (value will be multiplied by 100)
plot_learning_curves <- function(df,
ylim=c(-0.5,1.0),
legend_loc=c(0.95,0.75),
title="",
xlab="Number of initial training instances",
ylab="Average error relative to total elution time (%)",
percentage=F,
l1_selection=c("Layer 1","LASSO","BRR","SVR","AB","GB")){
# Percentages need to be multiplied by 100 %
if (percentage) {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf*100, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
else {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
# Apply a theme and put labels+legend in
p <- p + scale_fill_brewer("") + theme_classic() +
theme(plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
theme(axis.line.x = element_line(color="black", size = 0.5),
axis.line.y = element_line(color="black", size = 0.5),
axis.line = element_line(color="black", size = 0.5),
legend.justification=c(1,0), legend.position=legend_loc,legend.text=element_text(size=12))+
ylim(ylim)
p <- p + labs(title = title,
x = xlab,
y = ylab)
return(p)
}
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
library(ggplot2)
#########################################################################################
#                                                                                       #
# Plot the learning curves                                                              #
# Start with the aggregated learning curves (i.e. multiple learning curves in one plot) #
#                                                                                       #
#########################################################################################
# Function to plot the learning curve.
#
# Inputs:
#   df - dataframe consisting of the columns "experiment" (indicating the dataset)
#        "num_train" (indicating the number of training examples) "reDistributpeat" (indicating a possible repeat of learning)
#        "algo" (indicating the algorithm) "tr" (indicating the experimentally measured retention time)
#        "pred" (indicating the prediction)
#
#   ylim - vector indicating the limits c([from,to])
#
#   legend_loc - vector with location of the legend c([relative location x, relative location y])
#
#   title - string with the title
#
#   xlab - string with the x label
#
#   ylab - string with the y label
#
#   percentage - boolean indicating if percentages are used (value will be multiplied by 100)
plot_learning_curves <- function(df,
ylim=c(-0.5,1.0),
legend_loc=c(0.95,0.75),
title="",
xlab="Number of initial training instances",
ylab="Average error relative to total elution time (%)",
percentage=T,
l1_selection=c("Layer 1","LASSO","BRR","SVR","AB","GB")){
# Percentages need to be multiplied by 100 %
if (percentage) {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf*100, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
else {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
# Apply a theme and put labels+legend in
p <- p + scale_fill_brewer("") + theme_classic() +
theme(plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
theme(axis.line.x = element_line(color="black", size = 0.5),
axis.line.y = element_line(color="black", size = 0.5),
axis.line = element_line(color="black", size = 0.5),
legend.justification=c(1,0), legend.position=legend_loc,legend.text=element_text(size=12))+
ylim(ylim)
p <- p + labs(title = title,
x = xlab,
y = ylab)
return(p)
}
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
library(ggplot2)
#########################################################################################
#                                                                                       #
# Plot the learning curves                                                              #
# Start with the aggregated learning curves (i.e. multiple learning curves in one plot) #
#                                                                                       #
#########################################################################################
# Function to plot the learning curve.
#
# Inputs:
#   df - dataframe consisting of the columns "experiment" (indicating the dataset)
#        "num_train" (indicating the number of training examples) "reDistributpeat" (indicating a possible repeat of learning)
#        "algo" (indicating the algorithm) "tr" (indicating the experimentally measured retention time)
#        "pred" (indicating the prediction)
#
#   ylim - vector indicating the limits c([from,to])
#
#   legend_loc - vector with location of the legend c([relative location x, relative location y])
#
#   title - string with the title
#
#   xlab - string with the x label
#
#   ylab - string with the y label
#
#   percentage - boolean indicating if percentages are used (value will be multiplied by 100)
plot_learning_curves <- function(df,
ylim=c(-0.5,1.0),
legend_loc=c(0.95,0.75),
title="",
xlab="Number of initial training instances",
ylab="Average error relative to total elution time (%)",
percentage=T,
l1_selection=c("Layer 1","LASSO","BRR","SVR","AB","GB")){
# Percentages need to be multiplied by 100 %
if (percentage) {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf*100, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
else {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
# Apply a theme and put labels+legend in
p <- p + scale_fill_brewer("") + theme_classic() +
theme(plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
theme(axis.line.x = element_line(color="black", size = 0.5),
axis.line.y = element_line(color="black", size = 0.5),
axis.line = element_line(color="black", size = 0.5),
legend.justification=c(1,0), legend.position=legend_loc,legend.text=element_text(size=12))+
ylim(ylim)
p <- p + labs(title = title,
x = xlab,
y = ylab)
return(p)
}
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
xgboost <- c()
adaboost <- c()
lasso <- c()
l1 <- c()
bay_regr <- c()
comparison_algo$train_size <- as.numeric(comparison_algo$train_size )
for (e in unique(comparison_algo$experiment)){
temp_comparison_algo <- comparison_algo[comparison_algo$experiment==e,]
l1 <- c(l1,cor(temp_comparison_algo$rt,temp_comparison_algo$pred))
xgboost <- c(xgboost,cor(temp_comparison_algo$rt,temp_comparison_algo$XGBoost))
adaboost <- c(adaboost,cor(temp_comparison_algo$rt,temp_comparison_algo$Adaboost))
lasso <- c(lasso,cor(temp_comparison_algo$rt,temp_comparison_algo$Lasso))
bay_regr <- c(bay_regr,cor(temp_comparison_algo$rt,temp_comparison_algo$Bayesian_regression))
}
par(mar=c(10, 4, 4, 2) + 0.1)
par(mfrow=c(1,2))
#pdf(file="AlgorithmComparisonL1.pdf",width=6,height=6)
boxplot(lasso,bay_regr,adaboost,xgboost,l1,
names=c("Lasso","Bayesian regression","Adaboost","XGBoost","Layer 1"),
ylab="Pearson correlation",las=2)
algo_df <- data.frame("Lasso"=lasso,"Bayesian regression"=bay_regr,"Adaboost"=adaboost,"XGBoost"=xgboost)
count_best_perf <- table(colnames(algo_df)[apply(algo_df,1,which.max)])
barplot(count_best_perf[order(count_best_perf)],las=2,ylab="Best performing algorithm (#)")
#dev.off()
par(mar=c(4, 4, 4, 2) + 0.1)
par(mfrow=c(3,2))
combs_cols <- combn(colnames(algo_df),2)
for (i in 1:ncol(combs_cols)){
plot(algo_df[,combs_cols[,i]],xlim=c(0,1),ylim=c(0,1))
abline(0,1,lty=2)
}
library(ggplot2)
#########################################################################################
#                                                                                       #
# Plot the learning curves                                                              #
# Start with the aggregated learning curves (i.e. multiple learning curves in one plot) #
#                                                                                       #
#########################################################################################
# Function to plot the learning curve.
#
# Inputs:
#   df - dataframe consisting of the columns "experiment" (indicating the dataset)
#        "num_train" (indicating the number of training examples) "reDistributpeat" (indicating a possible repeat of learning)
#        "algo" (indicating the algorithm) "tr" (indicating the experimentally measured retention time)
#        "pred" (indicating the prediction)
#
#   ylim - vector indicating the limits c([from,to])
#
#   legend_loc - vector with location of the legend c([relative location x, relative location y])
#
#   title - string with the title
#
#   xlab - string with the x label
#
#   ylab - string with the y label
#
#   percentage - boolean indicating if percentages are used (value will be multiplied by 100)
plot_learning_curves <- function(df,
ylim=c(-0.5,1.0),
legend_loc=c(0.95,0.75),
title="",
xlab="Number of initial training instances",
ylab="Average error relative to total elution time (%)",
percentage=T,
l1_selection=c("Layer 1","LASSO","BRR","SVR","AB","GB")){
# Percentages need to be multiplied by 100 %
if (percentage) {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf*100, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
else {
p<-ggplot(df, aes(x=as.factor(number_train), y=perf, fill = algo)) +
geom_boxplot(position=position_dodge(0.6),width=0.65,outlier.size = 2.5) +
stat_summary(fun.y = mean, geom="point",colour="darkred", size=1,position=position_dodge(0.6))
}
# Apply a theme and put labels+legend in
p <- p + scale_fill_brewer("") + theme_classic() +
theme(plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
theme(axis.line.x = element_line(color="black", size = 0.5),
axis.line.y = element_line(color="black", size = 0.5),
axis.line = element_line(color="black", size = 0.5),
legend.justification=c(1,0), legend.position=legend_loc,legend.text=element_text(size=12))+
ylim(ylim)
p <- p + labs(title = title,
x = xlab,
y = ylab)
return(p)
}
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,0.3))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
comparison_algo_summary
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
comparison_algo_summary
l1_selection <-  c("Layer 1","Layer 2","Layer 3")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("Layer 1","Layer 2","Layer 3")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("Layer 1","Layer 2","Layer 3","Layer 4")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("Layer 1","Layer 2","Layer 3")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,30.0))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/nodup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,26.0))
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("Layer 1","Layer 2","Layer 3")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,26.0))
setwd("C:/Users/asus/Desktop/predictions/data/parsed/dup_preds_2019/")
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("LASSO","BRR","SVR","AB","GB","Layer 1")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,26.0))
comparison_algo_summary <- read.csv("results_avgerr_Summary.csv")
l1_selection <-  c("Layer 1","Layer 2","Layer 3")
comparison_algo_summary$algo <- factor(comparison_algo_summary$algo, levels = l1_selection)
plot_learning_curves(comparison_algo_summary[comparison_algo_summary$algo %in% l1_selection,],ylim=c(0.0,26.0))
